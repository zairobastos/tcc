{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transport = pd.read_csv('./data/Dados_Onibus_Fortaleza.csv', delimiter=',', encoding='UTF-8',low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>linha</th>\n",
       "      <th>data_hora</th>\n",
       "      <th>validations_per_hour</th>\n",
       "      <th>d_semana</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "      <th>hora</th>\n",
       "      <th>d_mes</th>\n",
       "      <th>d_ano</th>\n",
       "      <th>mes</th>\n",
       "      <th>semana_do_mes</th>\n",
       "      <th>domingo</th>\n",
       "      <th>segunda</th>\n",
       "      <th>terca</th>\n",
       "      <th>quarta</th>\n",
       "      <th>quinta</th>\n",
       "      <th>sexta</th>\n",
       "      <th>sabado</th>\n",
       "      <th>feriado</th>\n",
       "      <th>vespera_feriado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-01 01:00:00</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>0.269797</td>\n",
       "      <td>0.962917</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-01 02:00:00</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0.519584</td>\n",
       "      <td>0.854419</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   linha            data_hora  validations_per_hour  d_semana  hour_sin  \\\n",
       "0      1  2018-01-01 00:00:00                    29         0  0.000000   \n",
       "1      1  2018-01-01 01:00:00                    58         0  0.269797   \n",
       "2      1  2018-01-01 02:00:00                    42         0  0.519584   \n",
       "\n",
       "   hour_cos  hora  d_mes  d_ano  mes  semana_do_mes  domingo  segunda  terca  \\\n",
       "0  1.000000     0      1      1    1              1      1.0      0.0    0.0   \n",
       "1  0.962917     1      1      1    1              1      1.0      0.0    0.0   \n",
       "2  0.854419     2      1      1    1              1      1.0      0.0    0.0   \n",
       "\n",
       "   quarta  quinta  sexta  sabado  feriado  vespera_feriado  \n",
       "0     0.0     0.0    0.0     0.0        1                0  \n",
       "1     0.0     0.0    0.0     0.0        1                0  \n",
       "2     0.0     0.0    0.0     0.0        1                0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transport.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# caminho_modelo = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "# caminho_modelo = \"TheBloke/30B-Epsilon-AWQ\"\n",
    "caminho_modelo = \"TheBloke/laser-dolphin-mixtral-2x7b-dpo-AWQ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = f\"\"\"\n",
    "    Faça a extração dos nomes de todas as pessoas com as suas respectivas cor de cabelo e altura. \n",
    "    Em hipótese alguma extraia outro tipo de informação, extraia apenas nome, cor do cabelo e altura.\n",
    "    A sua saída deve retornar apenas um dicionário com os nomes das pessoas e suas respectivas informações.\n",
    "    Por exemplo, para o texto: \n",
    "    \"Alice é nadadora, mora em Fortaleza, tem um e cinquenta e possui cabelo loiro. Amanda é 15 centimetros mais alta que Alice e tem cabelo preto.\"\n",
    "    A saída deve ser:\n",
    "    Dicionário:\n",
    "    {{\n",
    "        \"Alice\": {{\"cor_do_cabelo\": \"loiro\", \"altura\": 1.5}},\n",
    "        \"Amanda\": {{\"cor_do_cabelo\": \"preto\", \"altura\": 1.65}}\n",
    "    }}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = f\"\"\"\n",
    "    Agora, extraia as informações para o texto a seguir:\n",
    "\n",
    "    \"Carlos é programador e tem 1 metro e 69 centimentros. Alex é gerente de banco e é 5 centimentros mais baixo que Carlos. \n",
    "    Ambos têm cabelo castanho. Já Cláudio joga basquete, ele tem cabelo preto e possui 1.9 metros.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"MIXTRAL\" in caminho_modelo.upper(): \n",
    "    prompt_template = f\"\"\"\n",
    "        <|im_start|>system\n",
    "        {system_message}\n",
    "        <|im_end|>\n",
    "        <|im_start|>user\n",
    "        {user_message}\n",
    "        <|im_end|>\n",
    "        <|im_start|>assistant\n",
    "        \\n\\n Dicionário:\n",
    "    \"\"\"\n",
    "\n",
    "    response_splitter = \"\\n\\n Dicionário:\"\n",
    "\n",
    "elif \"ZEPHYR\" in caminho_modelo.upper():\n",
    "\n",
    "    prompt_template = f\"\"\"\n",
    "        <|system|>\n",
    "        {system_message}</s>\n",
    "        <|user|>\n",
    "        {user_message}</s>\n",
    "        <|assistant|>\n",
    "        \\n\\n Dicionário:\n",
    "    \"\"\"\n",
    "\n",
    "    response_splitter = \"\\n\\n Dicionário:\"\n",
    "\n",
    "elif \"MISTRAL\" in caminho_modelo.upper():\n",
    "\n",
    "    prompt_template = f\"\"\"\n",
    "        [INST]\n",
    "        {system_message}\n",
    "        \n",
    "        {user_message}[/INST]\n",
    "        \\n\\n Dicionário:\n",
    "    \"\"\"\n",
    "\n",
    "    response_splitter = \"\\n\\n Dicionário:\"\n",
    "\n",
    "elif \"Epsilon\".upper() in caminho_modelo.upper():\n",
    "\n",
    "    prompt_template = f\"\"\"\n",
    "        ### Instruction:\n",
    "        {system_message}\n",
    "        \n",
    "        {user_message}\n",
    "        ### Response:\n",
    "        \\n\\n Dicionário:\n",
    "    \"\"\"\n",
    "\n",
    "    response_splitter = \"\\n\\n Dicionário:\"\n",
    "\n",
    "elif \"TESS\" in caminho_modelo.upper() or \"WhiteRabbitNeo\".upper() in caminho_modelo.upper():\n",
    "\n",
    "    prompt_template = f\"\"\"\n",
    "        SYSTEM:{system_message}        \n",
    "        USER:{user_message}\n",
    "        ASSISTANT:\n",
    "        \\n\\n Dicionário:\n",
    "    \"\"\"\n",
    "\n",
    "    response_splitter = \"\\n\\n Dicionário:\"\n",
    "\n",
    "else:\n",
    "\n",
    "    prompt_template = f\"\"\"\n",
    "        {system_message}        \n",
    "        {user_message}\n",
    "        \\n\\n Dicionário:\n",
    "    \"\"\"\n",
    "\n",
    "    response_splitter = \"\\n\\n Dicionário:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    torch.set_default_dtype(torch.float16)\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(caminho_modelo, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(caminho_modelo, device_map=device, trust_remote_code=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codifica o prompt de entrada\n",
    "tokens_entrada = tokenizer.encode(prompt_template, return_tensors='pt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = tokens_entrada.shape[1] + 100  # Ajuste conforme necessário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gera a máscara de atenção baseada nos tokens de entrada\n",
    "attention_mask = torch.ones(tokens_entrada.shape, dtype=torch.long, device=device)  # Cria uma máscara de atenção onde todos os tokens são considerados (1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Certifica-se de que o pad_token_id esteja definido explicitamente se não estiver\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar resumo com a máscara de atenção e pad_token_id definidos\n",
    "dict = model.generate(\n",
    "    input_ids=tokens_entrada,\n",
    "    attention_mask=attention_mask,  # Passa a máscara de atenção\n",
    "    max_length=max_length,\n",
    "    do_sample=True,\n",
    "    temperature=0.3,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id  # Garante que pad_token_id esteja definido explicitamente\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_entrada_saida = tokenizer.decode(dict[0], skip_special_tokens=True)\n",
    "\n",
    "texto_saida = texto_entrada_saida.split(response_splitter)[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('################## DICIONÁRIO ##################')\n",
    "print(texto_saida)\n",
    "print('################## DICIONÁRIO ##################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_saida = tokenizer.encode(texto_saida, return_tensors='pt').to(device)\n",
    "\n",
    "print(f\"Quantidade de tokens na entrada: {tokens_entrada.shape[1]}\")\n",
    "print(f\"Quantidade de tokens na saída: {tokens_saida.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "print(f\"Tempo de execução: {duration} segundos\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
